<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Evaluating Evidence and Making Decisions using Bayesian Statistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Mattan S. Ben-Shachar" />
    <meta name="date" content="2021-06-11" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"top-right","caseSensitive":false,"showIcon":false,"autoSearch":true}) })</script>
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="css/custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">








background-image: url(img/bg_main.png)
class: left, bottom, title-slide

# Evaluating Evidence and Making Decisions using Bayesian Statistics

## Methods Workshop @ DGMS

### Mattan S. Ben-Shachar

.right[
&lt;!-- .big[<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg> [.white[tinyurl.com/ISCoP-2021-bayes]](https://tinyurl.com/ISCoP-2021-bayes)]   --&gt;
&lt;!-- .big[<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M512 0C460.22 3.56 96.44 38.2 71.01 287.61c-3.09 26.66-4.84 53.44-5.99 80.24l178.87-178.69c6.25-6.25 16.4-6.25 22.65 0s6.25 16.38 0 22.63L7.04 471.03c-9.38 9.37-9.38 24.57 0 33.94 9.38 9.37 24.59 9.37 33.98 0l57.13-57.07c42.09-.14 84.15-2.53 125.96-7.36 53.48-5.44 97.02-26.47 132.58-56.54H255.74l146.79-48.88c11.25-14.89 21.37-30.71 30.45-47.12h-81.14l106.54-53.21C500.29 132.86 510.19 26.26 512 0z"/></svg> <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M593.8 59.1H46.2C20.7 59.1 0 79.8 0 105.2v301.5c0 25.5 20.7 46.2 46.2 46.2h547.7c25.5 0 46.2-20.7 46.1-46.1V105.2c0-25.4-20.7-46.1-46.2-46.1zM338.5 360.6H277v-120l-61.5 76.9-61.5-76.9v120H92.3V151.4h61.5l61.5 76.9 61.5-76.9h61.5v209.2zm135.3 3.1L381.5 256H443V151.4h61.5V256H566z"/></svg> [.white[Raw rmarkdown file]](https://github.com/mattansb/bayesian-evidence-iscop-2021/blob/main/bayesian-evidence-iscop-2021.Rmd)]   --&gt;
.big[Presented at June 10th, 2021]
&lt;!-- .small[(updated June 11, 2021)] --&gt;
]

---












class: title-slide, center

# About Me

&lt;img style="border-radius: 50%;" src="https://mattansb.github.io/CV/headshots/BrainOrange.jpg" width="150px"/&gt;

## Mattan S. Ben-Shachar

### PhD Student + Stats Lover + R Developer

.fade[Ben-Gurion University of the Negev&lt;br&gt;Beer Sheva, Israel]

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> [.white[@mattansb]](https://twitter.com/mattansb) | <svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> [.white[@mattansb]](https://github.com/mattansb)










---

background-color: var(--myred)
class: inverse

.center[
# About You
]

--

.big[
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M512 199.652c0 23.625-20.65 43.826-44.8 43.826h-99.851c16.34 17.048 18.346 49.766-6.299 70.944 14.288 22.829 2.147 53.017-16.45 62.315C353.574 425.878 322.654 448 272 448c-2.746 0-13.276-.203-16-.195-61.971.168-76.894-31.065-123.731-38.315C120.596 407.683 112 397.599 112 385.786V214.261l.002-.001c.011-18.366 10.607-35.889 28.464-43.845 28.886-12.994 95.413-49.038 107.534-77.323 7.797-18.194 21.384-29.084 40-29.092 34.222-.014 57.752 35.098 44.119 66.908-3.583 8.359-8.312 16.67-14.153 24.918H467.2c23.45 0 44.8 20.543 44.8 43.826zM96 200v192c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V200c0-13.255 10.745-24 24-24h48c13.255 0 24 10.745 24 24zM68 368c0-11.046-8.954-20-20-20s-20 8.954-20 20 8.954 20 20 20 20-8.954 20-20z"/></svg> You use statistical models (maybe in `R`)
- ANOVAs, regression
- Maybe some mixed models
]

--

.big[
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M512 199.652c0 23.625-20.65 43.826-44.8 43.826h-99.851c16.34 17.048 18.346 49.766-6.299 70.944 14.288 22.829 2.147 53.017-16.45 62.315C353.574 425.878 322.654 448 272 448c-2.746 0-13.276-.203-16-.195-61.971.168-76.894-31.065-123.731-38.315C120.596 407.683 112 397.599 112 385.786V214.261l.002-.001c.011-18.366 10.607-35.889 28.464-43.845 28.886-12.994 95.413-49.038 107.534-77.323 7.797-18.194 21.384-29.084 40-29.092 34.222-.014 57.752 35.098 44.119 66.908-3.583 8.359-8.312 16.67-14.153 24.918H467.2c23.45 0 44.8 20.543 44.8 43.826zM96 200v192c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V200c0-13.255 10.745-24 24-24h48c13.255 0 24 10.745 24 24zM68 368c0-11.046-8.954-20-20-20s-20 8.954-20 20 8.954 20 20 20 20-8.954 20-20z"/></svg> You've heard about (and maybe even used) Bayes factors
]

--

.big[
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M512 199.652c0 23.625-20.65 43.826-44.8 43.826h-99.851c16.34 17.048 18.346 49.766-6.299 70.944 14.288 22.829 2.147 53.017-16.45 62.315C353.574 425.878 322.654 448 272 448c-2.746 0-13.276-.203-16-.195-61.971.168-76.894-31.065-123.731-38.315C120.596 407.683 112 397.599 112 385.786V214.261l.002-.001c.011-18.366 10.607-35.889 28.464-43.845 28.886-12.994 95.413-49.038 107.534-77.323 7.797-18.194 21.384-29.084 40-29.092 34.222-.014 57.752 35.098 44.119 66.908-3.583 8.359-8.312 16.67-14.153 24.918H467.2c23.45 0 44.8 20.543 44.8 43.826zM96 200v192c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V200c0-13.255 10.745-24 24-24h48c13.255 0 24 10.745 24 24zM68 368c0-11.046-8.954-20-20-20s-20 8.954-20 20 8.954 20 20 20 20-8.954 20-20z"/></svg> **You want to know *more* about Bayesian stats**
]

---















class: middle, big

<svg aria-hidden="true" role="img" viewBox="0 0 576 512" style="height:1em;width:1.12em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M528 0H48C21.5 0 0 21.5 0 48v320c0 26.5 21.5 48 48 48h192l-16 48h-72c-13.3 0-24 10.7-24 24s10.7 24 24 24h272c13.3 0 24-10.7 24-24s-10.7-24-24-24h-72l-16-48h192c26.5 0 48-21.5 48-48V48c0-26.5-21.5-48-48-48zm-16 352H64V64h448v288z"/></svg> Link to this presentation:  
[tinyurl.com/bayesian-evidence/DGMS-2021](https://tinyurl.com/bayesian-evidence/DGMS-2021)

&lt;br&gt;

<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> All the code and materials used in this workshop can be found on GitHub:  
[github.com/mattansb/bayesian-evidence/DGMS-2021](https://github.com/mattansb/bayesian-evidence/DGMS-2021)


---

















# Outline

#### Part I: What is a Bayesian model?

- What is probability?

--

- How to Bayes, even?

--

- Why to Bayes? (aka "Why is this better than how I currently model?")

--

*(break)*

--

#### Part II: Evaluating Evidence and Making Decisions using Bayesian Statistics

- Demo: Building a Bayesian model
  - Posterior Estimates
  
--

*Let us begin...*

---









class: title-slide, center, middle

# Part I: What is a Bayesian Model?

### It's all About the &lt;br&gt; &lt;s&gt;Bass&lt;/s&gt; Bayesian Modeling

---









class: inverse

## What *is* a Bayesian model?

A Bayesian model is a statistical model where you use **probability** to represent **all uncertainty** within the model, both the uncertainty regarding the output but also the uncertainty regarding the input (aka parameters) to the model&lt;sup&gt;1&lt;/sup&gt;...

.footnote[
[1] Bååth (2015). *From [stackexchange](https://stats.stackexchange.com/a/129712/293056)*
]

--

How is this different from a non-Bayesian model? 🤔

???

- "uncertainty regarding the output" = how un/certain we are about our predictions.
- "uncertainty regarding the input" = how un/certain we are about our parameters.

---















class: big

## What *is* Probability?

There are *2* interpretations of the concept of probability:

--

- **Frequentists** (*Objective*) interpret probability as the *relative frequency* of occurrence of an experiment's outcome when the experiment is repeated indefinitely.

--

Results of a coin toss, winning the lotto, the probability of obtaining a result assuming `\(H_0\)` is correct, ...

--

- **Bayesians** (*Subjective*) interpret probability as the *relative degree of belief* in the possibility of an experiment's outcome to occur.

--

Weather forecasts, theory *A* being correct, do aliens exist, ...

???

Political forcasts fall under this type as well.

---














### Subjective???

.center[
&lt;img src="img/subjective.jpg" width="70%"/&gt;
]

.footnote[.small[.right[
[@ChelseaParlett](https://twitter.com/ChelseaParlett/status/1397962073422794752)
]]]

--

How do these beliefs come into play in a Bayesian *model*?

???

Subjective is not arbitrary - we each are experts in our own domain.
The probability of Gandhi coming back from the dead is pretty low - based on our subjective experience of the world. We are still grounded somewhat.

For example, I think there is a high probability of this workshop ending with some of you adopting Bayesian stats in your work.
Is that unreasonable? No. But it also does not make sense in a frequentist framework, because I am talking about *you* and *this* workshop specifically.

However it is true that different people can have different beliefs...

---

















class: title-slide, center, middle

## How to Bayes? <svg aria-hidden="true" role="img" viewBox="0 0 576 512" style="height:1em;width:1.12em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M208 0c-29.9 0-54.7 20.5-61.8 48.2-.8 0-1.4-.2-2.2-.2-35.3 0-64 28.7-64 64 0 4.8.6 9.5 1.7 14C52.5 138 32 166.6 32 200c0 12.6 3.2 24.3 8.3 34.9C16.3 248.7 0 274.3 0 304c0 33.3 20.4 61.9 49.4 73.9-.9 4.6-1.4 9.3-1.4 14.1 0 39.8 32.2 72 72 72 4.1 0 8.1-.5 12-1.2 9.6 28.5 36.2 49.2 68 49.2 39.8 0 72-32.2 72-72V64c0-35.3-28.7-64-64-64zm368 304c0-29.7-16.3-55.3-40.3-69.1 5.2-10.6 8.3-22.3 8.3-34.9 0-33.4-20.5-62-49.7-74 1-4.5 1.7-9.2 1.7-14 0-35.3-28.7-64-64-64-.8 0-1.5.2-2.2.2C422.7 20.5 397.9 0 368 0c-35.3 0-64 28.6-64 64v376c0 39.8 32.2 72 72 72 31.8 0 58.4-20.7 68-49.2 3.9.7 7.9 1.2 12 1.2 39.8 0 72-32.2 72-72 0-4.8-.5-9.5-1.4-14.1 29-12 49.4-40.6 49.4-73.9z"/></svg>

.bottom[To fit a Bayesian model you will need...]

---

### .blue[A Prior]

A probability distribution that represents your prior *belief* about the possible values each parameter can take.

--

&gt; *"But different people can have vastly different beliefs! How can we use this in **science**??"*  
&gt;     .center[\- You (2021)?]

--

In real life applications, you would be hard-pressed to just use whatever prior you like - you would need to somehow **justify your prior** (which requires domain specific knowledge - which you have!).

.footnote[

Watch also [Bürkner (2018). *Why not to be afraid of priors (too much)*](https://www.youtube.com/watch?v=Uz9r8eV2erQ)

]

--

...similar to how you must also justify the predictors, models, and everything else you use in your statistical analysis.

---









### .orange[A Likelihood Function]

What process best describes the data generation process? 
.small[What is the underlying distribution of our data?]

--

For example:
- A .orange[binomial] likelihood function for **binary** data
- A .orange[Poisson] likelihood function for **count** data
- A .orange[cumulative multinomial] likelihood function for **ordinal** data
- An .orange[inverse Gaussian / ex-Gaussian / [other]] likelihood function for **reaction times**
- ...
- A .orange[Gaussian] likelihood function for **conditionally normal** data

The likelihood function tells us the *probability of observing our data given the value(s) of some parameter(s)*.

--

This function is used to ***update the priors***, resulting in ***The Posterior***...

---









### Prior + Likelihhod = .green[Posterior]

This is that whole pesky *Bayes' Rule* thing everyone keeps going on about:

.content-box-green[
`$$\overbrace{P(\theta|Data)}^{\text{Posterior}} = \frac{\overbrace{P(Data|\theta)}^{\text{Likelihood}} \times \overbrace{P(\theta)}^{\text{Prior}}}{P(Data)}$$`

In words:

The **posterior probability** of some parameter `\(\theta\)` having a value of `\(x\)`, is a equal to probability of the observed data occurring if that were the value of `\(\theta\)` (**the likelihood**), normalized by our **prior belief** that `\(\theta\)` can have a value of `\(x\)`.
]

.footnote[
We usually can only estimate the posterior distribution by sampling from it.
]

---











&lt;!-- Normal Priors --&gt;

--

![](index_files/figure-html/normal_prior1a-1.png)&lt;!-- --&gt;

---

![](index_files/figure-html/normal_prior1b-1.png)&lt;!-- --&gt;

---

![](index_files/figure-html/normal_prior1c-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/normal_prior2-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/normal_prior3-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/normal_prior4-1.png)&lt;!-- --&gt;

---









&lt;!-- Weird Priors --&gt;

???

You will usually see some bell-curve-like prior, but priors can, in theory, take any weird shape you can think of.

--

![](index_files/figure-html/weird_prior1-1.png)&lt;!-- --&gt;

???

This is also called a horse-shoe prior - used for regularization.

--

![](index_files/figure-html/weird_prior2-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/weird_prior3-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/weird_prior4-1.png)&lt;!-- --&gt;

???

The posterior is not only affected by how strong or weak or priors are, but also by how strong/weak or data is...

---






















&lt;!-- Just likelihood --&gt;

![](index_files/figure-html/lik_plot1-1.png)&lt;!-- --&gt;

???

Stronger data = larger sample sizes and stronger effects - lead to more specific likelihood functions.

--

![](index_files/figure-html/lik_plot2-1.png)&lt;!-- --&gt;![](index_files/figure-html/lik_plot2-2.png)&lt;!-- --&gt;

---











background-color: var(--myred)
class: inverse
layout: true

## Why to Bayes?

### *AKA* "Why is this better than what I currently do?"

&lt;hr&gt;

---

--

- **Speak in the language of probabilities** (*probabilitese?*).

&gt; *There is a 0.2 (posterior) probability of the treatment alleviating more than 3 ADHD symptoms.*

&gt; *There is a 0.85 (posterior) probability of realibility of the test being at least `\(\alpha &gt; 0.8\)`.*

--

- **The power of Priors**

  - Utilize prior knowledge - *add* the information gained from the current data to the existing corpus of knowledge. 

      - Not every study is *tabula rasa*.

  - Use priors to prevent over-fitting (regularization via horseshoe, spike-and-slab).

???

We don't have to invent the wheel... And we can use priors to be cautious (regularization)...

---

**Fit complex models / to complex data**:

- Limiting the search space of our model's parameters to what is *a-priori* reasonable, reduces issues that plague other estimation methods.

  - failed convergence, local maxima, complete separation...
  
--

- With a likelihood function and a prior, you can add endless complexity to your model (even allow `\(n&lt;p\)`).
  
  - Easily model heteroscedasticity,
  - Model individual differences in ICC in HLM,
  - Easily obtain CIs for random effects,
  - ...

--

- **Some types of models cannot practically be analyzed using frequentists methods** 🤷 [(Rouder &amp; Lu, 2005)](https://twitter.com/Nate__Haines/status/1360227275711668228)

---





background-color: var(--myred)
layout: false
class: title-slide, middle, center

# Questions? &lt;br&gt; <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zM262.655 90c-54.497 0-89.255 22.957-116.549 63.758-3.536 5.286-2.353 12.415 2.715 16.258l34.699 26.31c5.205 3.947 12.621 3.008 16.665-2.122 17.864-22.658 30.113-35.797 57.303-35.797 20.429 0 45.698 13.148 45.698 32.958 0 14.976-12.363 22.667-32.534 33.976C247.128 238.528 216 254.941 216 296v4c0 6.627 5.373 12 12 12h56c6.627 0 12-5.373 12-12v-1.333c0-28.462 83.186-29.647 83.186-106.667 0-58.002-60.165-102-116.531-102zM256 338c-25.365 0-46 20.635-46 46 0 25.364 20.635 46 46 46s46-20.636 46-46c0-25.365-20.635-46-46-46z"/></svg>

---


layout: false
class: title-slide, middle, center

# Break <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M127.1 146.5c1.3 7.7 8 13.5 16 13.5h16.5c9.8 0 17.6-8.5 16.3-18-3.8-28.2-16.4-54.2-36.6-74.7-14.4-14.7-23.6-33.3-26.4-53.5C111.8 5.9 105 0 96.8 0H80.4C70.6 0 63 8.5 64.1 18c3.9 31.9 18 61.3 40.6 84.4 12 12.2 19.7 27.5 22.4 44.1zm112 0c1.3 7.7 8 13.5 16 13.5h16.5c9.8 0 17.6-8.5 16.3-18-3.8-28.2-16.4-54.2-36.6-74.7-14.4-14.7-23.6-33.3-26.4-53.5C223.8 5.9 217 0 208.8 0h-16.4c-9.8 0-17.5 8.5-16.3 18 3.9 31.9 18 61.3 40.6 84.4 12 12.2 19.7 27.5 22.4 44.1zM400 192H32c-17.7 0-32 14.3-32 32v192c0 53 43 96 96 96h192c53 0 96-43 96-96h16c61.8 0 112-50.2 112-112s-50.2-112-112-112zm0 160h-16v-96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48z"/></svg>

---











class: title-slide, center, middle

# Part II: Demo

Let's get our hands dirty...

???

Due to time constraints (fitting Bayesian models does take some time), I will walk you through the process of model fitting, exploration, and inference.

---








class: middle

We will be looking at a regression model, 

but the tools from this demo can be applied to Bayesian [SEM](https://faculty.missouri.edu/~merklee/blavaan/), IRT, SDT, [etc](https://cran.r-project.org/view=Bayesian)...

---














## The Data

500 individuals filled out a questionnaire about their lives during COVID-19 outbreak.

--

We will try to predict if individuals currently feel depressed (binary: Yes/No) with:

- How many times a week they've left the house (0-7)
- Are they employed (Yes/No)


- Controlling for age (0-99) and gender (Man/Woman).

---












We will be working in **`R`** with the following packages:

- `brms` for Bayesian Regression Models with *Stan*.

  - *Stan* is a probabilistic programming language

--

- `emmeans` for extracting estimates / contrasts / slopes from the model.

--

- `bayestestR` for descriptive and inferential statistics.

--

- Plots are made with `ggplot2` + `patchwork` + `tidybayes` + `ggdist` + `see`.

---











class: title-slide, middle, center

## Building a Bayesian Model

---



layout: true
class: small

### Choosing Priors

---


```r
my_priors &lt;- 
* set_prior("normal(0,4)", class = "Intercept") +
  # Covariables of interest 
  set_prior("student_t(3, 0, 0.1)", class = "b", coef = "going_out_c") + 
  set_prior("student_t(3, 0, 0.4)", class = "b", coef = "employed1") + 
  # Nuisance covariables
  set_prior("student_t(3, 0, 2)", class = "b", coef = "gender1") +
  set_prior("student_t(3, 0, 2)", class = "b", coef = "age_dec_c")
```

We will be using a logistic regression model, so all of our priors ar on the `\(log(Odds)\)` scale.

???

Logistic models are hard enough as it is... 

--

For the intercept (represents the overall odds/probability of depression) we will use a very wide prior - `\(log(Odds_{overall}) \sim Normal(0, 2)\)`.

???

The intercept represents the overall odds of depression.

--

On the probability scale, this translates to roughly a uniform prior between 0 and 1.

???

If we transform this prior from the log odds scale to the prob scale, this is roughly a uniform 0-1 prior.

---















```r
my_priors &lt;- 
  set_prior("normal(0,4)", class = "Intercept") + 
  # Covariables of interest 
* set_prior("student_t(3, 0, 0.1)", class = "b", coef = "going_out_c") +
* set_prior("student_t(3, 0, 0.4)", class = "b", coef = "employed1") +
  # Nuisance covariables 
* set_prior("student_t(3, 0, 2)", class = "b", coef = "gender1") +
* set_prior("student_t(3, 0, 2)", class = "b", coef = "age_dec_c")
```

.pull-left[
For our fixed effects, we will use a scaled `\(t_{(df=3)}\)`-prior centered on 0.

This prior has the benefit of the scaling factor giving the range where 60% of the prior's mass is.
]

???

For the other parameters in our model we will use a scaled t distribution - so it can be made wider or more narrow than a standard t dist.
I will be using a t dist with 3 df because the scaling factor gives the range in which 60% of the distribution lays.

--

.pull-left[
![](index_files/figure-html/t3-1.png)&lt;!-- --&gt;
]

--

.content-box-red[
**Note**: By default, `brms` sets **flat** (*diffused, extremely uninformative*) priors for fixed effects.
]











---


```r
my_priors &lt;- 
  set_prior("normal(0,4)", class = "Intercept") + 
  # Covariables of interest 
* set_prior("student_t(3, 0, 0.1)", class = "b", coef = "going_out_c") +
  set_prior("student_t(3, 0, 0.4)", class = "b", coef = "employed1") + 
  # Nuisance covariables 
  set_prior("student_t(3, 0, 2)", class = "b", coef = "gender1") + 
  set_prior("student_t(3, 0, 2)", class = "b", coef = "age_dec_c")
```

For `going_out` (centered) I will use a prior of `\(log(OR_\text{go out}) \sim t_{3}(0, 0.1)\)`.

--

**On the OR scale**: a 0.6 prior probability that odds of depression increase/decrease by *up to* a factor of 1.1-times-per-day.

???

I will use a t scaled by 0.1, centered on 0 (may increase, may decrease the odds).
In other words I am saying I have a prior of 0.6 that for every day one goes out, their odds of depression increase (or decrease) by a factor of up to 0.9-1.1 on the OR scale.










---


```r
my_priors &lt;- 
  set_prior("normal(0,4)", class = "Intercept") + 
  # Covariables of interest 
  set_prior("student_t(3, 0, 0.1)", class = "b", coef = "going_out_c") + 
* set_prior("student_t(3, 0, 0.4)", class = "b", coef = "employed1") +
  # Nuisance covariables 
  set_prior("student_t(3, 0, 2)", class = "b", coef = "gender1") + 
  set_prior("student_t(3, 0, 2)", class = "b", coef = "age_dec_c")
```

For `employed` we will use a prior of `\(log(OR_\text{employed}) \sim t_{3}(0, 0.4)\)`.

--

On the OR scale: a 0.6 prior probability that a being unemployed increase/decrease the odds of depression by *up to* a factor of 1.5.

???

I will use a t scaled by 0.4, centered on 0 (may increase, may decrease the odds).
In other words I am saying I have a prior of 0.6 that being unemployed increase/decrease the odds of depression by *up to* a factor of 0.7-1.5 on the OR scale.

---













```r
my_priors &lt;- 
  set_prior("normal(0,4)", class = "Intercept") + 
  # Covariables of interest 
  set_prior("student_t(3, 0, 0.1)", class = "b", coef = "going_out_c") + 
  set_prior("student_t(3, 0, 0.4)", class = "b", coef = "employed1") + 
  # Nuisance covariables 
* set_prior("student_t(3, 0, 2)", class = "b", coef = "gender1") +
* set_prior("student_t(3, 0, 2)", class = "b", coef = "age_dec_c")
```

For both age and gender (our nuisance parameters) - I don't have a strong or specific prior...
As far as I know they could have *any* effect on the probability of feeling more depressed during a pandemic.

--

We will set ***very wide priors*** of `\(\sim t_{3}(0, 2)\)`.

--

Translated ro odds:

- A prior of 0.6 that the odds of depression among men is *up to* 7.4 times more/less than women.

--

- A prior of 0.6 that the odds of depression increase/decrease by *up to* 7.4 times per decade of age (0.74 per year).

---













layout: false

### Fitting the Model


```r
m_depressed &lt;- brm(
  more_depressed ~ age_dec_c + gender + going_out_c + employed, 
  data = COVID_mental_health,
  prior = my_priors,
  family = binomial(link = "logit")
)
```

.big[
We are predicting increase in depression from age (in decades), gender, frequency of leaving the house (days-a-week) and employment.
]

--

.big[
The model is a logistic regression: a binomial model with a logit link function.
]

---







### Prior &amp; Posterior Checks

???

These checks just make sure that our model is very generally reasonable, and that we've done a good job of sampling from the posterior...

--

&lt;img src="img/footagenotfound.jpg" width="80%" /&gt;

.footnote[
See for example `bayestestR::diagnostic_posterior()` and `brms::pp_check()`...
]

---















layout: true

## Explore the Model

---




.pull-left[
Let's look at the posteriors of the estimated probability of an increase in depression for the Employed/Unemployed:


```r
resp_employed &lt;- 
  emmeans(m_depressed, ~ employed,
          trans = "response")
```

&lt;hr&gt;

]

---

.pull-left[
Let's look at the posteriors of the estimated probability of an increase in depression for the Employed/Unemployed:



```r
resp_employed &lt;- 
  emmeans(m_depressed, ~ employed,
          trans = "response")
```

&lt;hr&gt;

Frequentist estimation methods (such as **OLS** or **maximum likelihood (ML)**) produce a point estimate for each parameter.

But in Bayes we have not a single value, but .green[a whole distribution of values]!

We we can either .green[present the whole distribution, *as is*]...

]


--

.pull-right[

![](index_files/figure-html/plot_emply_probs-1.png)&lt;!-- --&gt;

]

---


















layout: false

Or we can summarize the posterior distribution:

--

.pull-left[

.purple[A Representative Value]

.small[(in lieu of a point estimate)]

- Median (most common)
- Mean
- Maximum A Posteriori (MAP)

]

--

.pull-right[
.red[Credible Intervals (CIs)]


- The Highest Density Interval (HDI; most common)
- The Equal-Tailed Interval (ETI)

]

--

&lt;hr&gt;


```r
describe_posterior(resp_employed, 
                   centrality = "median",
                   ci = 0.89, ci_method = "hdi",
                   test = NULL)
```

```
## Summary of Posterior Distribution
## 
## Parameter | Median |       89% CI
## ---------------------------------
## No        |   0.16 | [0.13, 0.19]
## Yes       |   0.12 | [0.08, 0.17]
```

---















background-color: var(--myred)
class: inverse, center, middle, title-slide

## Evaluating Evidence and Making Decisions &lt;br&gt; using Bayesian Statistics &lt;br&gt; <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

---














We are limiting our discussion to evaluating evidence for **single estimates / parameters** (expected values, slopes, contrasts...).

But it is also possible to evaluating evidence for multiple parameters, with order restrictions and model comparisons.

--

&lt;hr&gt;

We will be looking at two parameters - the effect of going out, and the effect of employment:


```r
POI &lt;- as.data.frame(m_depressed)[4:5]
```

.pull-left[

```r
describe_posterior(POI, ci = .89, # log(OR)
                   test = NULL) 
```

```
## Summary of Posterior Distribution
## 
## Parameter     | Median |         89% CI
## ---------------------------------------
## b_going_out_c |  -0.62 | [-1.09, -0.06]
## b_employed1   |  -0.21 | [-0.56,  0.17]
```
]

.pull-right[

```r
describe_posterior(exp(POI), ci = .89, # OR
                   test = NULL) 
```

```
## Summary of Posterior Distribution
## 
## Parameter     | Median |       89% CI
## -------------------------------------
## b_going_out_c |   0.54 | [0.27, 0.84]
## b_employed1   |   0.81 | [0.53, 1.12]
```
]

---
















class: small

### The Probability of Direction

- The maximal probability of the estimate being strictly directional (larger or smaller than 0).

- Generally ranges from 50% (no preference) to 100%.

--

.pull-left[


```r
p_direction(POI)
```

```
## Probability of Direction
## 
## Parameter     |     pd
## ----------------------
## b_going_out_c | 99.08%
## b_employed1   | 82.25%
```

]

.pull-right[

![](index_files/figure-html/pd_plot-1.png)&lt;!-- --&gt;

]

--

The there seems to be a high probability of direction for the relationship with *Going out*, but not that great for *Employment* ($p_d$ &lt; 0.95).

--

- &lt;b&gt;.green[Pros]&lt;/b&gt;: Easy to understand; "Resembles" the *p*-value - `\(r \simeq -1\)`. &lt;sup&gt;*&lt;/sup&gt;

- &lt;b&gt;.red[Cons]&lt;/b&gt;: like the *p*-values, a *low* `\(p_d\)` cannot be used to support the null.

---




















class: small

### *p*-MAP


- The *density ratio* between the null and the MAP value.

- Values range from 1 (the null *is* the MAP) to ~0 (the MAP is much much more probable than the null).

--

.pull-left[

```r
p_map(POI)
```

```
## MAP-based p-value
## 
## Parameter     | p (MAP)
## -----------------------
## b_going_out_c |   0.207
## b_employed1   |   0.703
```
]

.pull-right[

![](index_files/figure-html/pmap_plot-1.png)&lt;!-- --&gt;

]

--

For the effect of going out it seems like the MAP is more than 5 times more probable than the null. But for the effect of employment it is not even twice as probable. 

--

- &lt;b&gt;.green[Pros]&lt;/b&gt;: Closely related to LRT tests - familiar; Also closely associated with the *p*-value.

- &lt;b&gt;.red[Cons]&lt;/b&gt;: Again, a *high* *p*-MAP cannot be used to support the null.

---




















### *p*-ROPE

- The probability that our estimate is *basically* null.

--

- We first define a **Region of Practical Equivalence (ROPE)** - a range of effects that are, for any practical purposes, the same as no effect at all. 

--

For the both parameters, we will define any change smaller than an OR of 1.05 to be considered just as good as no change at all - so ROPE is `\(log([0.95, 1.05]) = [-0.05, 0.05]\)`.

--

.small[We can also have a one sided ROPE, with [-Inf, +0.05], etc.]

---

class: small

### *p*-ROPE

- How much of the posterior falls in the ROPE.

  - Or: How much of the most probable values (e.g., those in the HDI) fall in the ROPE.

--

.pull-left[


```r
rope(POI, 
     range = c(-0.05, 0.05), ci = 0.89)
```

```
## # Proportion of samples inside the ROPE [-0.05, 0.05]:
## 
## Parameter   | inside ROPE
## -------------------------
## going_out_c |      0.00 %
## employed1   |     13.54 %
```

]

.pull-right[

![](index_files/figure-html/therope_plot-1.png)&lt;!-- --&gt;

]

--

It is very improbable that the relationship with *Going out* is very small. *But* there is about a 14% that *Employment* is practically unrelated to an increase in depression. [Though not very conclusive](https://easystats.github.io/bayestestR/articles/guidelines.html#significance)) we are supporting the null (at least a bit)!

???

If you've ever heard that "Bayes is good for small samples" this is what is meant by that: that unlike frequentist methods where small samples and non-significant results leave you high and dry, Bayes allows you to same *something*, weak as it may be.

---
















The *p*&lt;sub&gt;*d*&lt;/sub&gt;, *p*-MAP and ROPE are **posterior based methods** - they inform us about the accumulated information in the .blue[priors] + .orange[our data].

--

Often we are interested in **what has been *learned* in the current study, from the current data**.

--

.pull-left[

E.g., by now it is well established that *leaving the house* is generally good for mental health (exercise, socializing, sunlight...). But **which values of the effect of going out are supported or contradicted by the *current* data?** Maybe our data supports the null value(s) - what can be learnt from that?

To answer these types of questions we can *compare* .blue[The Prior] to .green[The Posterior] and see .orange[what our data taught us] - what values became more / less plausible.

]

--


.pull-right[

![](index_files/figure-html/support_plot-1.png)&lt;!-- --&gt;

]

--

We can use this information to look at different *sets of parameter values* - or **hypotheses** - E.g. `\(H_{small}: \theta \in [-1.1, 1.1]\)`, `\(H_{positive}: \theta \in [0, \infty]\)` and ask:

&gt; Which *hypothesis* is supported *more* by the data?

---











class: small

### The Bayes Factor

This index of evidence is a ***Bayes Factor***:

- It quantifies how the prior was *updated* to the posterior.

- It compares two "hypotheses".

--

**Any** measure that quantifies this 👆 is a Bayes factor.

.content-box-red[
There are many different type of questions that can be answered with Bayes factors - we will be looking at two.
]

???

You maybe have used Bayes factors to compare between models... Those too have these properties.

--

&lt;hr&gt;
For technical reasons we need a model that represents *only our priors* - which we will then *compare* to the results from our updated (posterior) model.

We can do that with the `unupdate()` function:


```r
# Get the priors only ("un-update" the model).
m_depressed_prior &lt;- unupdate(m_depressed)
POI_prior &lt;- as.data.frame(m_depressed_prior)[4:5]
```




---



















layout: true

### The Null-Interval Bayes Factor

---

The null-interval Bayes factor is an extension of the ROPE test;

--

&gt; How has the *relative probability*&lt;sup&gt;[1]&lt;/sup&gt; of the the effect being practically null changed? Does the data support or contradict the effect being null?

.footnote[
[1] The odds of the effect being inside the ROPE to it being outside the ROPE.
]

--

&lt;hr&gt;

The two hypotheses we will be comparing, using the same ROPE:

- `\(H_0: \text{log(effect)} \in [-0.05, +0.05]\)`  
- `\(H_A: \text{log(effect)} \notin [-0.05, +0.05]\)`  
  - Or: `\(H_A: \text{effect} &lt; -0.05\)` or `\(+0.05 &lt; \text{effect}\)`
















---

class: small

--

.pull-left[


```r
bayesfactor_parameters(
  POI, 
  prior = POI_prior,
  null = c(-0.05, 0.05) # same ROPE as before
)
```

```
## Bayes Factor (Null-Interval)
## 
## Parameter     |    BF
## ---------------------
## b_going_out_c | 19.48
## b_employed1   | 0.715
## 
## * Evidence Against The Null: [-0.050, 0.050]
```

]

--

.pull-right[

![](index_files/figure-html/bf_ROPE_plot-1.png)&lt;!-- --&gt;

]

--

- For the effect of *Going out*, the ROPE has *become* much less probable - with the data giving ~20 times more support for non-ROPE values.

--

- For the effect of *Employment*, the ROPE has *become* somewhat **more** probable - with the data giving (1/0.7 =) 1.4 times more support compared to the non-ROPE values.

---
























layout: true

### The Point-Null Bayes Factor

---

The point-null can be thought of as the null-interval Bayes factor with an infinitesimally small ROPE - that includes only one null value, exactly.

--

&gt; How has the probability&lt;sup&gt;[1,2]&lt;/sup&gt; of the the null value changed? Does the data support or contradict the effect being null?

This Bayes factor is also called the *Savage-Dickey density ratio*, .small[and it is analogous to a Bayes factor comparing two nested models.]

.footnote[

[1] Actually the density of the null.  
[2] This is also relative - if the null became more probable, necessarily the non-null values became less, and vice versa.

]

--

&lt;hr&gt;

The two hypotheses we will be comparing:

- `\(H_0: \text{effect} = 0\)`  
- `\(H_A: \text{effect} \neq 0\)`  
  - Or: `\(H_A: \text{effect} &lt; 0\)` or `\(0 &lt; \text{effect}\)`

---















class: small

--

.pull-left[


```r
bayesfactor_parameters(
  POI, 
  prior = POI_prior,
  null = 0
)
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter     |    BF
## ---------------------
## b_going_out_c | 14.43
## b_employed1   | 0.741
## 
## * Evidence Against The Null: 0
```

]

--

.pull-right[

![](index_files/figure-html/bf_point_plot-1.png)&lt;!-- --&gt;

]

--


- For the effect of *Going out*, the *mass* of the posterior is shifted *away* from the null (compared to the prior) - the data giving ~14 times more support for non-null values.

--

- For the effect of *Employment*, the mass has moved *towards* 0, the data giving (1/0.74 =) 1.35 times more support compared to the non-null values.

---

class: small

.pull-left[


```r
bayesfactor_parameters(
  POI, 
  prior = POI_prior,
  null = 0
)
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter     |    BF
## ---------------------
## b_going_out_c | 14.43
## b_employed1   | 0.741
## 
## * Evidence Against The Null: 0
```

]

.pull-right[


```r
bayesfactor_parameters(
  POI, 
  prior = POI_prior,
  null = c(-0.05, 0.05) # same ROPE as before
)
```

```
## Bayes Factor (Null-Interval)
## 
## Parameter     |    BF
## ---------------------
## b_going_out_c | 19.48
## b_employed1   | 0.715
## 
## * Evidence Against The Null: [-0.050, 0.050]
```

]

.content-box-green[

Here the point-null and the null-interval BFs gave similar results, but that need not be the case - depending on the effect size, the definition of the ROPE, the sample size, etc - these can give very different results!

]

---












layout: false

### Other Bayes Factors

- **Directional** null-interval / point-null Bayes factors

  - e.g., [-0.05, +0.05] *vs* [+0.05, Inf]

- Bayes factor for **dividing hypotheses**

  - e.g., [-Inf, 0] *vs* [0, Inf]

- **Model restricted** Bayes factors
  
- And more...

Read more about these Bayes factors [here](https://easystats.github.io/bayestestR/articles/bayes_factors.html)!

---















layout: true

## Age

---

--

.pull-left[

For *covariates*, we can present the posterior distribution of slopes, but be can also present a *trace plot* of slopes from the posterior.

.small[For example, we can sample 100 slopes from the posterior, and plot each one:]

]

--

.pull-right[

![](index_files/figure-html/age_lines_plot-1.png)&lt;!-- --&gt;

]

---

.pull-left[

Here too we can summarize the posterior distribution:


```r
slope_age &lt;- as.data.frame(m_depressed)[2]

describe_posterior(slope_age, ci = .89,
                   test = NULL)
```

```
## Summary of Posterior Distribution
## 
## Parameter   | Median |        89% CI
## ------------------------------------
## b_age_dec_c |   0.08 | [-0.18, 0.35]
```

]

--

.pull-right[

![](index_files/figure-html/slopes_standard_plot-1.png)&lt;!-- --&gt;

]

---














layout: false

#### *p*-Direction &amp; *p*-MAP

--

.pull-left[


```r
p_direction(slope_age)
```

```
## Probability of Direction
## 
## Parameter   |     pd
## --------------------
## b_age_dec_c | 68.70%
```

]


--


.pull-right[


```r
p_map(slope_age)
```

```
## MAP-based p-value
## 
## Parameter   | p (MAP)
## ---------------------
## b_age_dec_c |   0.886
```

]

--

&lt;br&gt;&lt;br&gt;

Not very decisive... (remember, these cannot be used to support the null!)

---














#### *p*-ROPE

For the ROPE,  think any change smaller than an OR of 1.15 to be considered just as good as no change at all - so ROPE is `\(log([0.87, 1.15]) = [-0.14, 0.14]\)` (you may disagree...):

--


```r
rope(slope_age, range = c(-0.14, 0.14), ci = 0.89)
```

```
## # Proportion of samples inside the ROPE [-0.14, 0.14]:
## 
## Parameter | inside ROPE
## -----------------------
## age_dec_c |     62.90 %
```

--

There is about a 64% probability that the effect of age on reaction times is *practically* nothing!

Not strongly conclusive, but at the very least it is suggestive!

---













layout: true
class: small

#### Bayes Factor

---

--


```r
bayesfactor_parameters(
  slope_age,
  prior = as.data.frame(m_depressed_prior)[2],
  null = 0
)
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter   |    BF
## -------------------
## b_age_dec_c | 0.080
## 
## * Evidence Against The Null: 0
```

Wow! It seems that the data strongly support (by a factor of 1/0.08 = 12.5) the effect of age being null over it being non-null!

--

*But wait* - the Bayes factor measures the change from the prior to the posterior… But what was our prior here?

---

.pull-left[


```r
bayesfactor_parameters(
  slope_age,
  prior = as.data.frame(m_depressed_prior)[2],
  null = 0
)
```

```
## Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter   |    BF
## -------------------
## b_age_dec_c | 0.080
## 
## * Evidence Against The Null: 0
```

]

--

.pull-right[

![](index_files/figure-html/slopes_BF_plot-1.png)&lt;!-- --&gt;

]

--

We used a super vague prior - which give some non-trivial probability to extreme effects!

So is it really surprising that the posterior is now, relatively closer to the *null*? ***No.***

???

In our prior, the null was very very importable - it is not therefore surprising that it became *more* probable.

--

.content-box-red[
With wide and uninformative enough priors, the Bayes factor will **always favor the null / ROPE**!  
DO NOT COMPUTE BAYES FACTORS WITH UNINFORMATIVE PRIORS! &lt;sup&gt;*&lt;/sup&gt;
]

???

This is only an issue if one of your hypotheses is a point, or almost a point. 

---












background-color: var(--myred)
class: inverse
layout: false

# Recommendations

### *What to actually report?*

We (Makowski et al., 2019) [recommend](https://easystats.github.io/bayestestR/articles/guidelines.html) reporting for inferential statistics:

--

- **The *p*-direction**: Easy to understand, easy to "translate" to *p*-values.

--

- ***p*-ROPE**: Provides information about the practical relevance of the effect, and allows to accept the null.

--

*If* informed priors are used,

- **Bayes factor** .small[(instead or in addition to the *p*-ROPE)]: Provides information about hypotheses supported or contradicted by the data.

---














class: title-slide

# Summary

### *What you now know! <svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/></svg>*

- What a Bayesian model *is*.

- What Bayes can give you, that no one else can.

- A taste of Bayesian model fitting with `brms`.

- The richness of inferences that can be made with Bayesian statistics.

---










class: title-slide, small

# Suggested Reading

#### For Bayesian Beginners

- Makowski, D., Ben-Shachar, M. S., Chen, S. H., &amp; Lüdecke, D. (2019). [Indices of effect existence and significance in the Bayesian framework. *Frontiers in psychology, 10*, 2767.](https://doi.org/10.3389/fpsyg.2019.02767)

  - [`bayestestR` guides and articles.](https://easystats.github.io/bayestestR)

- Van de Schoot, R. et al (2021). [Bayesian statistics and modelling. *Nature Reviews Methods Primers, 1*(1), 1-26.](https://doi.org/10.1038/s43586-020-00001-2)

- [Bayesian Inference for Psychology. *Psychonomic Bulletin and Review*.](https://scholar.google.co.il/scholar?q=Bayesian+inference+for+psychology+Psychonomic+Bulletin+and+Review)

#### Books

- Kruschke, J. (2014). Doing bayesian data analysis: A tutorial with r, jags, and stan. Academic Press.

- McElreath, R. (2018). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.

  - [Richard's YouTube channel](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA)

---








background-color: var(--myred)
class: inverse

.pull-left[
&lt;img style="border-radius: 50%;" src="https://mattansb.github.io/CV/headshots/BrainOrange.jpg" width="150px"/&gt;&lt;img src="img/BGU-logo-round-clear.png" width="20%" /&gt;&lt;img src="img/lab_logo.png" width="20%" /&gt;

# Thank you!

### Follow me!

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> [.white[@mattansb]](https://twitter.com/mattansb) | <svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> [.white[@mattansb]](https://github.com/mattansb) | <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;fill-opacity:white;position:relative;"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"/></svg> [.white[Blog]](https://shouldbewriting.netlify.com/)

]


.pull-right[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
.center[
&lt;img src="img/easystats.png" width="30%" /&gt;&lt;img src="img/bayestestR.png" width="30%" /&gt;
]

.small[
The [**`bayestestR`**](https://easystats.github.io/bayestestR) package is part of the `easystats` project. Core team members:

- Me 👋

- Dominique Makowski ([@Dom_Makowski](https://twitter.com/Dom_Makowski))

- Daniel Lüdecke ([@strengejacke](https://twitter.com/strengejacke))

- Indrajeet Patil ([@patilindrajeets](https://twitter.com/patilindrajeets))
]
]

.footnote[
*Slides created with the R package [**`xaringan`**](https://github.com/yihui/xaringan).*
]

---









background-image: url(img/boyfriend2.jpg)
class: right, bottom

[.black[@kareem_carr]](https://twitter.com/kareem_carr/status/1356986263975395333)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
