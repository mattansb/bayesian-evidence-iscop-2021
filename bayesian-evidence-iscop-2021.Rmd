---
title: "Evaluating Evidence and Making Decisions using Bayesian Statistics"
subtitle: "ISCoP Conference 2021"  
author: "Mattan S. Ben-Shachar"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["css/xaringan-themer.css", "hygge", "css/custom.css"]
    seal: false
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
knit: (
  function(inputFile, encoding, ...) { 
    rmarkdown::render( 
      input       = inputFile, 
      encoding    = encoding,
      output_file = "index", 
      ...) })
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
opts_chunk$set(
  echo = FALSE,
  dpi = 600,
  warning = FALSE,
  message = FALSE
)
library(fontawesome)
# library(xaringanExtra)
# library(xaringanBuilder)

# actual stats
library(brms)
library(bayestestR)
options(contrasts = c('contr.bayes', 'contr.bayes'))
library(logspline)

library(emmeans)

library(dplyr)
library(ggplot2)
library(ggnewscale)
library(patchwork)
library(tidybayes)
library(ggdist)
library(see)
theme_set(theme_ggdist())
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
link_color <- "#bf94e4"
style_duo_accent(
  primary_color = "#182933",
  secondary_color = "#a2174a",
  # Header
  inverse_header_color = "#FFFFFF",
  header_color = "#000000",
  # Text
  inverse_text_color = "#FFFFFF",
  text_color = "#000000",
  # Others
  text_bold_color = NULL,
  link_color = link_color,
  code_inline_color = NULL,
  # MISC
  colors = c(myblue = "#182933",
             myred = "#a2174a"),
  outfile = "css/xaringan-themer.css", 
)
```

```{r xaringanExtra-search, echo=FALSE}
xaringanExtra::use_search(position = "top-right")
```

background-image: url(img/bg_main.png)
class: left, bottom, title-slide

# Evaluating Evidence and Making Decisions using Bayesian Statistics

## ISCoP Conference 2021

### Mattan S. Ben-Shachar

.right[
.big[`r fa("link",,"white")` [.white[tinyurl.com/ISCoP-2021-bayes]](https://tinyurl.com/ISCoP-2021-bayes)]  
.big[`r fa("feather-alt",,"white")` `r fa("markdown",,"white")` [.white[Raw rmarkdown file]](https://github.com/mattansb/bayesian-evidence-iscop-2021/blob/main/bayesian-evidence-iscop-2021.Rmd)]  
.big[Presented at February 23, 2021]  
.small[(updated `r format(Sys.Date(), "%B %d, %Y")`)]
]

---












class: title-slide, center

# About Me

<img style="border-radius: 50%;" src="https://mattansb.github.io/CV/headshots/BrainOrange.jpg" width="150px"/>

## Mattan S. Ben-Shachar

### PhD Student + Stats Lover + R Developer

.fade[Ben-Gurion University of the Negev<br>Beer Sheva, Israel]

`r fa("twitter",,"white")` [.white[@mattansb]](https://twitter.com/mattansb) | `r fa("github",,"white")` [.white[@mattansb]](https://github.com/mattansb)










---

background-color: var(--myred)
class: inverse

.center[
# About You
]

--

.big[
`r fa("hand-point-right",,"white")` You use statistical models (in `R`)
- ANOVAs, regression
- Maybe some mixed models
]

--

.big[
`r fa("hand-point-right",,"white")` You've heard about (and maybe even used) Bayes factors
]

--

.big[
`r fa("hand-point-right",,"white")` **You want to know *more* about Bayesian stats**
]

---















class: middle, big

`r fa("desktop")` Link to this presentation:  
[tinyurl.com/ISCoP-2021-bayes](https://tinyurl.com/ISCoP-2021-bayes)

<br>

`r fa("github")` All the code and materials used in this workshop can be found on GitHub:  
[github.com/mattansb/bayesian-evidence-iscop-2021](https://github.com/mattansb/bayesian-evidence-iscop-2021)

---

















# Outline

--

- What is a Bayesian model?

--

- How to Bayes, even?

--

- Why to Bayes? (aka "Why is this better than how I currently model?")

--

- Demo: Building a Bayesian model
  - Posterior Estimates
  
--

  - **Evaluating Evidence and Making Decisions using Bayesian Statistics**

--

*Let us begin...*

---









class: title-slide, center, middle

# It's all About the <br> <s>Bass</s> Bayesian Modeling

---









class: inverse

# What *is* a Bayesian model?

A Bayesian model is a statistical model where you use **probability** to represent **all uncertainty** within the model, both the uncertainty regarding the output but also the uncertainty regarding the input (aka parameters) to the model<sup>1</sup>...

.footnote[
[1] Bååth (2015). *From [stackexchange](https://stats.stackexchange.com/a/129712/293056)*
]

???

- "uncertainty regarding the output" = how un/certain we are about our predictions.
- "uncertainty regarding the input" = how un/certain we are about our parameters.

--

... where probability expresses *a degree of belief* in an event.

---






class: title-slide, center, middle

# How to Bayes? `r fa("brain",,"white")`

.bottom[To fit a Bayesian model you need...]

---

### .blue[A Prior]

A probability distribution representing your prior *belief* about the probability of possible values each parameter can take.

--

> *"Sounds too subjective to be used in Science!"*  
>     .center[\- You (2021)?]

--

In real life applications, you would be hard-pressed to just use whatever prior you like - you would need to somehow **justify your prior** (which requires domain specific knowledge).

.footnote[

Watch also [Bürkner (2018). *Why not to be afraid of priors (too much)*](https://www.youtube.com/watch?v=Uz9r8eV2erQ)

]

--

Similar to how you must also justify and use a reasonable likelihood function.

---









### .orange[A Likelihood Function]

What process best describes the (conditional) data generation process? 

--

For example:
- A .orange[binomial] likelihood function for **binary** data
- A .orange[Poisson] likelihood function for **count** data
- A .orange[cumulative multinomial] likelihood function for **ordinal** data
- An .orange[inverse Gaussian / ex-Gaussian / [other]] likelihood function for **reaction times**
- ...
- A .orange[Gaussian] likelihood function for **conditionally normal** data

The likelihood function tells us the *probability of observing our data given the value(s) of some parameter(s)*.

--

This function is used to ***update the priors***, resulting in ***The Posterior***...

---









### Prior + Likelihhod = .green[Posterior]

This is that whole pesky *Bayes' Rule* thing everyone keeps going on about:

.content-box-green[
$$\overbrace{P(\theta|Data)}^{\text{Posterior}} = \frac{\overbrace{P(Data|\theta)}^{\text{Likelihood}} \times \overbrace{P(\theta)}^{\text{Prior}}}{P(Data)}$$

In words:

The **posterior probability** of some parameter $\theta$ having a value of $x$, is a equal to probability of the observed data occurring if that were the value of $\theta$ (**the likelihood**), normalized by our **prior belief** that $\theta$ can have a value of $x$.
]

.footnote[
We usually can only estimate the posterior distribution by sampling from it.
]

---









```{r plot_bayes_function}
plot_bayes <- function(dPrior, dLikelihood,
                       xlim = c(-10, 10), ylim = 0.4, header = FALSE) {
  dPost <- function(x, fPrior, fLikelihood, norm = TRUE) {
    d <- fPrior(x) * fLikelihood(x)
    if (norm) 
      d <- d / integrate(dPost, -Inf, Inf,
                         fPrior = fPrior, 
                         fLikelihood = fLikelihood,
                         norm = FALSE)[["value"]]
    d
  }
  
  p1 <- ggplot() +
    stat_function(fun = dPrior, xlim = xlim,
                  size = 1, color = "blue") +
    stat_function(fun = dPrior, xlim = xlim, 
                  size = 1, fill = "blue", geom = "area", alpha = 0.4) +
    labs(y = if (header) "Density",
         x = if (header) expression(theta),
         title = if(header) "Prior")
  
  p2 <- ggplot() +
    stat_function(fun = dLikelihood, xlim = xlim, 
                  size = 1, color = "orange2") +
    stat_function(fun = dLikelihood, xlim = xlim, 
                  size = 1, fill = "orange2", geom = "area", alpha = 0.4) +
    labs(y = NULL,
         title = if(header) "Likelihood")
  
  p3 <- ggplot() +
    stat_function(aes(color = "Prior"), fun = dPrior, xlim = xlim, 
                  size = 1, alpha = 0.4) +
    stat_function(aes(fill = "Prior"), fun = dPrior, xlim = xlim, 
                  size = 1, geom = "area", alpha = 0.1) +
    stat_function(aes(color = "Likelihood"), fun = dLikelihood, xlim = xlim, 
                  size = 1, alpha = 0.4,) +
    stat_function(aes(fill = "Likelihood"), fun = dLikelihood, xlim = xlim, 
                  size = 1, geom = "area", alpha = 0.1) +
    stat_function(aes(color = "Posterior"), fun = dPost, xlim = xlim, 
                  size = 1,
                  args = list(fPrior = dPrior, fLikelihood = dLikelihood)) +
    stat_function(aes(fill = "Posterior"), fun = dPost, xlim = xlim, 
                  size = 1,
                  args = list(fPrior = dPrior, fLikelihood = dLikelihood),
                  geom = "area", alpha = 0.4) +
    labs(y = NULL, color = NULL,
         title = if(header) "Posterior") +
    scale_color_manual(values = c(Prior = "blue",
                                  Likelihood = "orange2",
                                  Posterior = "darkgreen"),
                       aesthetics = c("fill", "color"),
                       breaks = c("Prior", "Likelihood", "Posterior"))
  
  p1 + p2 + p3 &
    coord_cartesian(ylim = c(0, ylim)) &
    theme_minimal() &
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_blank(),
          legend.position = "none")
}

dLikelihood <- function(x) dnorm(x, 1.5, sqrt(5))
# dPrior3 <- function(x) dnorm(x, 6 + 1.5, sqrt(25))
```

<!-- Normal Priors -->

--

```{r normal_prior1a, fig.width = 16, fig.height = 2.3}
dPrior1 <- function(x) rep(.Machine$double.xmin, length(x))
p_flat <- plot_bayes(dPrior1, dLikelihood, header = TRUE)
p_flat[[1]] + plot_spacer() + plot_spacer()
```

---

```{r normal_prior1b, fig.width = 16, fig.height = 2.3}
p_flat[[1]] + p_flat[[2]] + plot_spacer()
```

---

```{r normal_prior1c, fig.width = 16, fig.height = 2.3}
p_flat
```

--

```{r normal_prior2, fig.width = 16, fig.height = 2}
dPrior2 <- function(x) dnorm(x, 0 - 1.5, sqrt(25))
plot_bayes(dPrior2, dLikelihood)
```

--

```{r normal_prior3, fig.width = 16, fig.height = 2}
dPrior4 <- function(x) dnorm(x, 2.5 + 1.5, sqrt(5))
plot_bayes(dPrior4, dLikelihood)
```

--

```{r normal_prior4, fig.width = 16, fig.height = 2}
dPrior5 <- function(x) dnorm(x, 2.5 + 1.5, sqrt(0.2))
plot_bayes(dPrior5, dLikelihood, ylim = 1)
```

---









<!-- Weird Priors -->

???

You will usually see some bell-curve-like prior, but priors can, in theory, take any weird shape you can think of.

--

```{r weird_prior1, fig.width = 16, fig.height = 2.3}
dPrior8 <- function(x) dexp(abs(x), rate = 1/2) / 2
plot_bayes(dPrior8, dLikelihood, header = TRUE)
```

???

This is also called a horse-shoe prior - used for regularization.

--

```{r weird_prior2, fig.width = 16, fig.height = 2}
dPrior9 <- function(x, norm = TRUE) {
  d <- dcauchy(x)
  d[x > -2 & x < 2] <- 0
  
  if (norm) 
    d <- d / integrate(dPrior9, -Inf, Inf,
                       norm = FALSE)[["value"]]
  d
}
plot_bayes(dPrior9, dLikelihood, ylim = 0.7)
```

--

```{r weird_prior3, fig.width = 16, fig.height = 2}
dPrior6 <- function(x, norm = TRUE) {
  d <- dnorm(x, 3, 3)
  d[x > 3] <- d[x > 3] * 3
  if (norm) 
    d <- d / integrate(dPrior6, -Inf, Inf,
                       norm = FALSE)[["value"]]
  d
}
plot_bayes(dPrior6, dLikelihood, ylim = 0.35)
```

--

```{r weird_prior4, fig.width = 16, fig.height = 2}
dPrior7 <- function(x, norm = TRUE) {
  d <- (cos(x*1.5) + 1.1) * dnorm(x, mean = 0, sd = 4)
  if (norm) 
    d <- d / integrate(dPrior7, -Inf, Inf,
                       norm = FALSE)[["value"]]
  d
}
plot_bayes(dPrior7, dLikelihood)
```

???

The posterior is not only affected by how strong or weak or priors are, but also by how stong/weak or data is...

---






















<!-- Just likelihood -->

```{r lik_plot1, fig.width = 16, fig.height = 2.3}
plot_bayes(dPrior4, dLikelihood) +
  plot_annotation(title = "N = 30")
```

???

Stronger data = larger sample sizes and stronger effects - lead to more specific likelihood functions.

--

```{r lik_plot2, fig.width = 16, fig.height = 2.3}
plot_bayes(dPrior4, function(x) dnorm(x, 1.5, sqrt(5/2))) +
  plot_annotation(title = "N = 60")

plot_bayes(dPrior4, function(x) dnorm(x, 1.5, sqrt(5/10)), ylim = .6) +
  plot_annotation(title = "N = 300")
```

---











background-color: var(--myred)
class: inverse
layout: true

# Why to Bayes?

### *AKA* "Why is this better than what I currently do?"

<hr>

---

--

- **Speak in the language of probabilities** (*probabilitese?*).

> *There is a 0.2 (posterior) probability of the treatment alleviating more than 3 ADHD symptoms.*

> *There is a 0.85 (posterior) probability of realibility of the test being at least $\alpha > 0.8$.*

--

- **The power of Priors**

  - Utilize prior knowledge - *add* the information gained from the current data to the existing corpus of knowledge. 

      - Not every study is *tabula rasa*.

  - Use priors to prevent over-fitting (regularization via horseshoe, spike-and-slab).

???

We don't have to invent the wheel... And we can use priors to be cautious (regularization)...

---

**Fit complex models / to complex data**:

- Limiting the search space of our model's parameters to what is *a-priori* reasonable, reduces issues that plague other estimation methods.

  - failed convergence, local maxima, complete separation...
  
--

- With a likelihood function and a prior, you can add endless complexity to your model (even allow $n<p$).
  
  - Easily model heteroscedasticity,
  - Model individual differences in ICC in HLM,
  - Easily obtain CIs for random effects,
  - ...

--

- **Some types of models cannot practically be analyzed using frequentists methods** `r emo::ji("shrug")` [(Rouder & Lu, 2005)](https://twitter.com/Nate__Haines/status/1360227275711668228)

---







layout: false
class: title-slide, bottom

# Demo

## Let's get our hands dirty...

.right[
See the full analysis script [here `r fa("link",,link_color)`](https://mattansb.github.io/bayesian-evidence-iscop-2021/files/full%20analysis%20script.nb.html)
]

???

Due to time constraints (fitting Bayesian models does take some time), I will walk you through the process of model fitting, exploration, and inference.

---

class: middle

We will be looking at a regression model, 

but the tools from this demo can be applied to Bayesian [SEM](https://faculty.missouri.edu/~merklee/blavaan/), IRT, SDT, [etc](https://cran.r-project.org/view=Bayesian)...

---










class: small

## The Data

--

Thirty 4 year old children completed **Flanker's task**. .small[(real data.)]

.pull-left[
.center[
**Congruent**

<img src="img/flanker_fishC.png" width="80%"/>
]
]

.pull-right[
.center[
**Incongruent**

<img src="img/flanker_fishI.png" width="80%"/>
]
]

.center[
**Neutral**

<img src="img/flanker_fishN.png" width="40%"/>
]

.footnote[
*Image from [pixy](https://pixy.org/4293048/).*
]

--

We will be examining their **Interference** (Incongruent - Neutral) and **Facilitation** (Neutral - Congruent) effects, **controlling for age** (in months).

---










We will be working in **`R`** with the following packages:

- `brms` for Bayesian Regression Models with *Stan*.

  - *Stan* is a probabilistic programming language

--

- `emmeans` for extracting estimates / contrasts / slopes from the model.

- `bayestestR` for descriptive and inferential statistics.

--

- Plots are made with `ggplot2` + `patchwork` + `tidybayes` + `ggdist` + `see`.



.footnote[
[See other package versions and packages used `r fa("link",,link_color)`](https://mattansb.github.io/bayesian-evidence-iscop-2021/files/full%20analysis%20script.nb.html#setup)
]

---











class: small
layout: true

## Building a Bayesian Model

---

```{r the_model, eval = FALSE, echo = TRUE}
m_flanker <- brm(
  RT ~ Congruency + age_mo + (Congruency | id), #<<
  data = child_flanker, #<<
  prior = 
    # Two parameters for Congruency 
    set_prior("student_t(3, 0, 100)", class = "b",
              coef = c("Congruency1", "Congruency2")) +
    # Slope of age_mo
    set_prior("student_t(3, 0, 1000)", class = "b",
              coef = "age_mo"),
  family = gaussian())
```

We will be fitting an hierarchical linear model - predicting (single trial) RTs from `Congruency` (I, N, C) which is nested within each child (`id`) - controlling for the children's age (in months, `age_mo`).

This is essentially a repeated measures ANCOVA.

--

.content-box-red[
Note: For `Congruency` I've used ***orthonormal* dummy-coding**. This is important, but `r emo::ji("hourglass_not_done")`! Read more about that [here](https://easystats.github.io/bayestestR/articles/bayes_factors.html#contr_bayes).
]

---

```{r model_priors, eval = FALSE, echo = TRUE}
m_flanker <- brm(
  RT ~ Congruency + age_mo + (Congruency | id), 
  data = child_flanker, 
  prior = #<<
    # Two parameters for Congruency #<<
    set_prior("student_t(3, 0, 100)", class = "b", #<<
              coef = c("Congruency1", "Congruency2")) + #<<
    # Slope of age_mo #<<
    set_prior("student_t(3, 0, 1000)", class = "b", #<<
              coef = "age_mo"), #<<
  family = gaussian())
```

For our fixed effects, we will be somewhat conservative and use a scaled *t*(3)-prior centered on 0. This prior has the benefit of the scaling factor giving the range where 60% of the prior's mass is.

--

- In adults, the Flanker effect is about 20-50ms. Here we have 4yo - reasonable (?) that any differences between means (effect) would be **~100ms**, which we will use as our scaling factor ([Jonkman et al, 1999](https://doi.org/10.1111/1469-8986.3640419)).

--

- Prior on effect of age - no idea. We will use a weakly informative prior scaled to 1000ms/month (covering a very large range of possible effects).

---

```{r, ref.label="model_priors", eval = FALSE, echo = TRUE}

```

.content-box-red[
Notes:
  - By default, `brms` sets **flat** (*diffused, extremely uninformative*) priors for fixed effects.
  - We can also set a prior of `sigma` (error variance), and many others. See more options with `brms::get_prior()`.
]

---

```{r model_lik, eval = FALSE, echo = TRUE}
m_flanker <- brm(
  RT ~ Congruency + age_mo + (Congruency | id), 
  data = child_flanker, 
  prior = 
    # Two parameters for Congruency 
    set_prior("student_t(3, 0, 100)", class = "b",
              coef = c("Congruency1", "Congruency2")) + 
    # Slope of age_mo 
    set_prior("student_t(3, 0, 1000)", class = "b",
              coef = "age_mo"), 
  family = gaussian()) #<<
```

We will be using a Gaussian likelihood function of $RT \sim N(\mu_i, \sigma^2)$, where $\mu_i =a + \sum b_j X_{ij}$.

AKA, a boring linear regression.

---









layout: false

### Prior & Posterior Checks

???

These checks just make sure that our model is very generally reasonable, and that we've done a good job of sampling from the posterior...

--

<img src="img/footagenotfound.jpg" width="80%" />

.footnote[
But you can find them, and more, in [the full analysis script `r fa("link",,link_color)`](https://mattansb.github.io/bayesian-evidence-iscop-2021/files/full%20analysis%20script.nb.html)...
]

---









layout: true

## Explore the Model

---

```{r load_data_and_models}
child_flanker <- readRDS("files/child_flanker.rds")
m_flanker <- readRDS("files/m_flanker.rds")
m_flanker_prior <- readRDS("files/m_flanker_prior.rds")
```


.pull-left[
Let's look at the posteriors of the estimated means for the Congruency conditions:

```{r cong_means, echo = TRUE}
means_Congruency <-
  emmeans(m_flanker, ~ Congruency)
```

<hr>

]

---

.pull-left[
Let's look at the posteriors of the estimated means for the Congruency conditions:

```{r, ref.label="cong_means", eval = FALSE, echo = TRUE}
```

<hr>

Frequentist estimation methods (such as **OLS** or **maximum likelihood (ML)**) produce a point estimate for each parameter.

But in Bayes we have not a single value, but .green[a whole distribution of values]!

We we can either .green[present the whole distribution, *as is*]...

]


--

.pull-right[

```{r plot_cong_means, fig.width=4, fig.height=4}
means_for_plots <- gather_emmeans_draws(means_Congruency)

p_means <- ggplot(means_for_plots, aes(Congruency, .value)) +
  labs(y = "RT")
  
p1 <- p_means + 
  stat_slab()

p2 <- p_means + 
   stat_gradientinterval(thickness = 1,
                         color = NA, # no interval
                         fill = "gray2")

p1 + (p2 + theme(axis.title.y = element_blank(),
                 axis.text.y = element_blank())) &
  scale_x_discrete(guide = guide_axis(n.dodge=2))
```

]

---







layout: false

Or we can summarize the posterior distribution:

--

.pull-left[

.purple[A Representative Value]

.small[(in lieu of a point estimate)]

- Median (most common)
- Mean
- Maximum A Posteriori (MAP)

]

--

.pull-right[
.red[Credible Intervals (CIs)]


- The Highest Density Interval (HDI; most common)
- The Equal-Tailed Interval (ETI)

]

--

<hr>

```{r describe_post, echo = TRUE}
describe_posterior(means_Congruency, 
                   centrality = "median",
                   ci = 0.89, ci_method = "hdi",
                   test = NULL)
```

---











background-color: var(--myred)
class: inverse, center, middle, title-slide

## Evaluating Evidence and Making Decisions <br> using Bayesian Statistics <br> `r fa("search",,"white")`

---











We are limiting our discussion to evaluating evidence for **single estimates / parameters** (expected values, slopes, contrasts...).

But it is also possible to evaluating evidence for multiple parameters, with order restrictions and model comparisons. (Maybe next year...)

--

<hr>

We will be looking at two contrasts: the Interference and Facilitation effects:

```{r contrasts, echo=TRUE}
diffs_Congruency <- contrast(means_Congruency, 
                             list(Interference = c(1, -1, 0),
                                  Facilitation = c(0,  1, -1)))

describe_posterior(diffs_Congruency, test = NULL)
```


---











class: small

### The Probability of Direction

- The maximal probability of the estimate being strictly directional (larger or smaller than 0).

- Generally ranges from 50% (no preference) to 100%.

--

.pull-left[

```{r pd, echo = TRUE}
p_direction(diffs_Congruency)
```

]

.pull-right[

```{r pd_plot, fig.width=4, fig.height=2}
diff_draws <- gather_emmeans_draws(diffs_Congruency)

ggplot(diff_draws, aes(.value, contrast)) +
  stat_slab(aes(fill = after_stat(x > 0))) +
  scale_fill_brewer("", type = "qual", labels = c("Negative", "Positive")) +
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0)) + 
  coord_cartesian(xlim = c(-250, 400)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = NULL, x = NULL)
```

]

--

For the Interference effect it seems like these is a high probability of direction, but not that great for the Facilitation effect ( $p_d$ < 0.95 ). 

--

- <b>.green[Pros]</b>: Easy to understand; Resembles the *p*-value - $r \simeq -1$. <sup>*</sup>

- <b>.red[Cons]</b>: like the *p*-values, a *low* $p_d$ cannot be used to support the null.

---















class: small

### *p*-MAP


- The *density ratio* between the null and the MAP value.

- Values range from 1 (the null *is* the MAP) to ~0 (the MAP is much much more probable than the null).

--

.pull-left[

```{r pmap, echo = TRUE}
p_map(diffs_Congruency)
```

]

.pull-right[

```{r pmap_plot, fig.width=4, fig.height=2}
diff_density <- diff_draws %>% 
  group_by(contrast) %>% 
  summarise(estimate_density(.value))

dmap <- diff_draws %>% 
  group_by(contrast) %>% 
  summarise(x = c(map_estimate(.value), 0),
            y = density_at(.value, x))
  
ggplot(diff_density, aes(x, contrast)) +
  geom_slab(aes(thickness = y)) +
  geom_point(aes(x = x, y = 135 * y + as.numeric(factor(contrast)),
                 color = x == 0), data = dmap, size = 2) +
  geom_segment(aes(x = x, yend = y + as.numeric(factor(contrast)),
                   xend = x, y = 135 * y + as.numeric(factor(contrast)),
                   color = x == 0), data = dmap, size = 1) + 
  scale_color_manual(NULL, values = c("purple", "red"), labels = c("MAP", "null")) + 
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0)) + 
  coord_cartesian(xlim = c(-250, 400)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = NULL, x = NULL)
```

]

--

For the Interference effect it seems like the MAP is more th 10 times more probable than the null. But for the Facilitation effect it is not even twice as probable. 

--

- <b>.green[Pros]</b>: Closely related to LRT tests - familiar; Also closely associated with the *p*-value.

- <b>.red[Cons]</b>: Again, a *high* *p*-MAP cannot be used to support the null.

---














### *p*-ROPE

- The probability that our estimate is *basically* null.

--

- We first define a **Region of Practical Equivalence (ROPE)** - a range of effects that are, for any practical purposes, the same as no effect at all. 

--

For the Congruency effects, we will define any effect that is smaller in magnitude than 30ms, to be consider to be just as good as no effect at all - so ROPE [-30, +30].

--

.small[We can also have a one sided ROPE, with [-Inf, +30], etc.]

---

class: small

### *p*-ROPE

- How much of the posterior falls in the ROPE.

  - Or: How much of the most probable values (e.g., those in the HDI) fall in the ROPE.

--

.pull-left[

```{r therope, echo=TRUE}
rope(diffs_Congruency, 
     range = c(-30, 30), ci = 0.89)
```

]

.pull-right[

```{r therope_plot, fig.width=4, fig.height=2}
ggplot(diff_draws, aes(.value, contrast)) +
  stat_slab(aes(fill = after_stat(cut_cdf_qi(cdf, .width = c(0.89, 1)))), color = "gray") + 
  scale_fill_brewer("CI", direction = -1) +
  annotate("rect", xmin = -30, xmax = 30, ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4) + 
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0)) + 
  coord_cartesian(xlim = c(-250, 400)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = NULL, x = NULL)
```

]

--

It is very improbable that the Interference effect is very small. *But* there is about a 30% that among 4 year olds, there is no Facilitation effect - ([though not very conclusive](https://easystats.github.io/bayestestR/articles/guidelines.html#significance)) we are supporting the null!

???

If you've ever heard that "Bayes is good for small samples" this is what is meant by that: that unlike frequentist methods where small samples and non-significant results leave you high and dry, Bayes allows you to same *something*, weak as it may be.

---














The *p*<sub>*d*</sub>, *p*-MAP and ROPE are **posterior based methods** - they inform us about the accumulated information in the priors + our data.

--

Often we are interested in **what has been *learned* in the current study, from the current data**.

--

.pull-left[

E.g., by now it's clear that there exists an Interference effect in Flanker's task. But **which values of the effect are supported or contradicted by the *current* data?** Maybe our data supports the null value(s) - what can be learnt from that?

To answer these types of questions we can *compare* .blue[The Prior] to .green[The Posterior] to see .orange[what our data taught us] - what values became more / less plausible.

]

--


.pull-right[

```{r support_plot, fig.width=4, fig.height=2.5}
x <- seq(-10, 10, length.out = 1001)
dPost <- dnorm(x, 3, sqrt(1.5))
dPrior <- dnorm(x, 0, sqrt(10))

d <- data.frame(
  x = c(x, x),
  thickness = c(dPrior, dPost), 
  type = factor(rep(c("Prior", "Posterior"), each = 1001), 
                levels = c("Prior", "Posterior"))
)

ggplot() +
  geom_slab(aes(x = x, y = 1, thickness = thickness, 
                fill = type, color = type),
            data = d, alpha = .4) +
  scale_color_manual("DIstribution", values = c("blue", "darkgreen"),
                     aesthetics = c("fill", "color")) +
  new_scale_color() +
  geom_line(aes(x = x, y = 0.9, color = dPost > dPrior), size = 2) +
  scale_color_manual("Support",
                     values = c("orange", "red"),
                     labels = c("Lost", "Gained"))+
  scale_y_continuous(NULL,
                     breaks = c(0.9, 1),
                     labels = c("Support", "Distribution"))+
  labs(x = expression(theta)) +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.line.y = element_blank())
```

]

--

We can use this information to look at different *sets of parameter values* - or **hypotheses** - E.g. $H_{small}: \theta \in [-3, 3]$, $H_{positive}: \theta \in [0, \infty]$ and ask:

> Which *hypothesis* is supported *more* by the data?

---


















### The Bayes Factor

This index of evidence is a ***Bayes Factor***:

- It quantifies how the prior was *updated* to the posterior.

- It compares two "hypotheses".

--

**Any** measure that quantifies this `r emo::ji("backhand_index_pointing_up")` is a Bayes factor.

.content-box-red[

There are many different type of questions that can be answered with Bayes factors - we will be looking at two.

]

???

You maybe have used Bayes factors to compare between models... Those too have these properties.

--

<hr>

For technical reasons we need a model that represents *only our priors* - which we will then *compare* to the results from our updated (posterior) model.

We can do that with the `unupdate()` function:

```{r unupdate, echo=TRUE, eval=FALSE}
# Get the priors only ("un-update" the model).
m_flanker_prior <- unupdate(m_flanker)
```

---







layout: true

### The Null-Interval Bayes Factor

---

The null-interval Bayes factor is an extension of the ROPE test;

--

> How has the *relative probability*<sup>[1]</sup> of the the effect being practically null changed? Does the data support or contradict the effect being null?

.footnote[
[1] The odds of the effect being inside the ROPE to it being outside the ROPE.
]

--

<hr>

The two hypotheses we will be comparing, using the same ROPE:

- $H_0: \text{effect} \in [-30, +30]$  
- $H_A: \text{effect} \notin [-30, +30]$  
  - Or: $H_A: \text{effect} < -30$ or $+30 < \text{effect}$
















---

class: small

--

.pull-left[

```{r bf_ROPE, echo=TRUE}
bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = c(-30, 30) # same ROPE as before
)
```

]

--

.pull-right[

```{r bf_ROPE_plot, fig.width=4, fig.height=3}
BFpoint <- bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = 0 # same ROPE as before
)

BFplot_data <- attr(BFpoint, "plot_data")

ggplot(BFplot_data$plot_data, aes(x, ind, fill = Distribution, color = Distribution)) +
  geom_slab(aes(thickness = y), alpha = 0.4) +
  annotate("rect", fill = "red", alpha = 0.4,
           xmin = -30, xmax = 30, ymin = -Inf, ymax = Inf) +
  scale_fill_manual(NULL, 
                    values = c(prior = "blue", posterior = "darkgreen"), 
                    aesthetics = c("fill", "color")) + 
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0), limits = rev(levels(BFplot_data$plot_data$ind))) + 
  coord_cartesian(xlim = c(-500, 500)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = NULL, x = NULL)
```

]

--

- For the Interference effect, the ROPE has *become* relatively less probable - with the data giving 6 times more support for non-ROPE values.

--

- For the Facilitation effect, the ROPE has *become* relatively **more** probable - with the data giving (1/0.5 =) 2 times more support compared to the non-ROPE values.

---









layout: true

### The Point-Null Bayes Factor

---

The point-null can be thought of as the null-interval Bayes factor with an infinitesimally small ROPE - that includes only one null value, exactly.

--

> How has the probability<sup>[1,2]</sup> of the the null value changed? Does the data support or contradict the effect being null?

This Bayes factor is also called the *Savage-Dickey density ratio*, .small[and it is analogous to a Bayes factor comparing two nested models.]

.footnote[

[1] Actually the density of the null.  
[2] This is also relative - if the null became more probable, necessarily the non-null values became less, and vice versa.

]

--

<hr>

The two hypotheses we will be comparing:

- $H_0: \text{effect} = 0$  
- $H_A: \text{effect} \neq 0$  
  - Or: $H_A: \text{effect} < 0$ or $0 < \text{effect}$

---















class: small

--

.pull-left[

```{r bf_point, echo=TRUE}
bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = 0
)
```

]

--

.pull-right[

```{r bf_point_plot, fig.width=4, fig.height=3}
ggplot(BFplot_data$plot_data, aes(x, ind, fill = Distribution, color = Distribution)) +
  geom_slab(aes(thickness = y), alpha = 0.4) +
  geom_point(aes(x = x, y = 135 * y + 3 - as.numeric(factor(ind)), fill = Distribution),
             data = BFplot_data$d_points, size = 1.5, color = "red", shape = 21, stroke = 1.5) + 
  scale_fill_manual(NULL, 
                    values = c(prior = "blue", posterior = "darkgreen"), 
                    aesthetics = c("fill", "color")) + 
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0), limits = rev(levels(BFplot_data$plot_data$ind))) +
  coord_cartesian(xlim = c(-500, 500)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = NULL, x = NULL)
```

]

--

- For the Interference effect, the *mass* of the posterior is shifted *away* from the null (compared to the prior) - the data giving ~6 times more support for non-null values.

--

- For the Facilitation effect the mass has moved *towards* 0, the data giving (1/0.6 =) 1.7 times more support compared to the non-null values.

---

class: small

.pull-left[

```{r, ref.label="bf_point", echo=TRUE}

```

]

.pull-right[

```{r, ref.label="bf_ROPE", echo=TRUE}

```

]

.content-box-green[

Here the point-null and the null-interval BFs gave similar results, but that need not be the case - depending on the effect size, the definition of the ROPE, the sample size, etc.

]

---

















layout: false

### Other Bayes Factors

- **Directional** null-interval / point-null Bayes factors

  - e.g., [-30, +30] *vs* [+30, Inf]

- Bayes factor for **dividing hypotheses**

  - e.g., [-Inf, 0] *vs* [0, Inf]

- **Model restricted** Bayes factors

  - [Incongruent > Neutral > Congruent] vs [Incongruent ≠ Neutral ≠ Congruent]
  
- And more...

Read more about these Bayes factors [here](https://easystats.github.io/bayestestR/articles/bayes_factors.html)!

---












layout: true

## Age

---

--

.pull-left[

For *covariates*, we can present the posterior distribution of slopes, but be can also present a *trace plot* of slopes from the posterior.

.small[For example, we can sample 100 slopes from the posterior, and plot each one:]

]

--

.pull-right[

```{r age_lines_plot, fig.width=4, fig.height=4}
slope_for_plots2 <- emmeans(m_flanker, ~age_mo, cov.red = unique) %>% 
  gather_emmeans_draws() %>% 
  ungroup()

set.seed(1)
slope_for_plots2 %>% 
  # sample only 200 slopes
  group_nest(.draw) %>%
  sample_n(100) %>%
  tidyr::unnest(data) %>% 
  ggplot(aes(age_mo, .value, group = .draw)) +
  geom_line(alpha = 0.2, size = 1) +
  # scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = "RT", x = "Age [months]")
```

]

---

.pull-left[

Here too we can summarize the posterior distribution:

```{r slopes, echo = TRUE}
slope_age <- emtrends(m_flanker, ~1, "age_mo")

describe_posterior(slope_age, test = NULL)
```

]

--

.pull-right[

```{r slopes_standard_plot, fig.width=4, fig.height=4}
slope_for_plots2 %>% 
  ggplot(aes(age_mo, .value)) +
  stat_lineribbon(point_interval = median_hdi,
                  .width = c(0.5, 0.89, 0.99)) +
  scale_fill_brewer("HDI") +
  scale_y_discrete(expand = c(0.1, 0)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = "RT", x = "Age [months]")
```

]

---














layout: false

#### *p*-Direction & *p*-MAP

--

.pull-left[

```{r slopes_pd, echo = TRUE}
p_direction(slope_age)
```

]


--


.pull-right[

```{r slopes_pd_pmap, echo = TRUE}
p_map(slope_age)
```

]

--

<br><br>

Not very decisive… (remember, these cannot be used to support the null!)

---














#### *p*-ROPE

For the ROPE - I think any effect smaller an overall change of less than 500ms a year = ~40ms a month, is practically 0 (you may disagree…):

--

```{r slopes_rope, echo = TRUE}
rope(slope_age, range = c(-40, 40), ci = 0.89)
```

--

There is about a 60% probability that the effect of age on reaction times is *practically* nothing!

Not strongly conclusive, but at the very least it is suggestive!

---













layout: true
class: small

#### Bayes Factor

---

--

```{r slopes_BF, echo=TRUE}
bayesfactor_parameters(
  slope_age,
  prior = m_flanker_prior,
  null = 0
)
```

Wow! It seems that the data strongly support (by a factor of 1/0.05 = 20) the effect of age being null over it being non-null!

--

*But wait* - the Bayes factor measures the change from the prior to the posterior… But what was our prior here?

---

.pull-left[

```{r, ref.label="slopes_BF", echo=TRUE}

```

]

--

.pull-right[

```{r slopes_BF_plot, fig.width=4, fig.height=2.6}
BFpoint <- bayesfactor_parameters(
  slope_age, 
  prior = m_flanker_prior,
  null = 0 # same ROPE as before
)

BFplot_data <- attr(BFpoint, "plot_data")

ggplot(BFplot_data$plot_data, aes(x, ind, fill = Distribution, color = Distribution)) +
  geom_slab(aes(thickness = y), alpha = 0.4) +
  geom_point(aes(x = x, y = 1 + 105*y, fill = Distribution),
             data = BFplot_data$d_points, size = 1.5, color = "red", shape = 21, stroke = 1.5) +
  scale_fill_manual(NULL, 
                    values = c(prior = "blue", posterior = "darkgreen"), 
                    aesthetics = c("fill", "color")) + 
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_discrete(expand = c(0.1, 0)) + 
  coord_cartesian(xlim = c(-2500, 2500)) + 
  theme(legend.position = "bottom") +
  labs(title = NULL, y = NULL, x = NULL)
```

]

--

We used a super vague prior - which give some non-trivial probability to extreme effects!

So is it really surprising that the posterior is now, relatively closer to the *null*? ***No.***

???

In our prior, the null was very very importable - it is not therefore surprising that it became *more* probable.

--

.content-box-red[
With wide and uninformative enough priors, the Bayes factor will **always favor the null / ROPE**!  
DO NOT COMPUTE BAYES FACTORS WITH UNINFORMATIVE PRIORS! <sup>*</sup>
]

???

This is only an issue if one of your hypotheses is a point, or almost a point. 

---















background-color: var(--myred)
class: inverse
layout: false

# Recommendations

### *What to actually report?*

We (Makowski et al., 2019) [recommend](https://easystats.github.io/bayestestR/articles/guidelines.html) reporting for inferential statistics:

--

- **The *p*-direction**: Easy to understand, easy to "translate" to *p*-values.

--

- ***p*-ROPE**: Provides information about the practical relevance of the effect, and allows to accept the null.

--

*If* informed priors are used,

- **Bayes factor** .small[(instead or in addition to the *p*-ROPE)]: Provides information about hypotheses supported or contradicted by the data.

---














class: title-slide

# Summary

### *What you now know! `r fa("graduation-cap",,"white")`*

- What a Bayesian model *is*.

- What Bayes can give you, that no one else can.

- A taste of Bayesian model fitting with `brms`.

- The richness of inferences that can be made with Bayesian statistics.

---










class: title-slide, small

# Suggested Reading

#### For Bayesian Beginners

- Makowski, D., Ben-Shachar, M. S., Chen, S. H., & Lüdecke, D. (2019). [Indices of effect existence and significance in the Bayesian framework. *Frontiers in psychology, 10*, 2767.](https://doi.org/10.3389/fpsyg.2019.02767)

  - [`bayestestR` guides and articles.](https://easystats.github.io/bayestestR)

- Van de Schoot, R. et al (2021). [Bayesian statistics and modelling. *Nature Reviews Methods Primers, 1*(1), 1-26.](https://doi.org/10.1038/s43586-020-00001-2)

- [Bayesian Inference for Psychology. *Psychonomic Bulletin and Review*.](https://scholar.google.co.il/scholar?q=Bayesian+inference+for+psychology+Psychonomic+Bulletin+and+Review)

#### Books

- Kruschke, J. (2014). Doing bayesian data analysis: A tutorial with r, jags, and stan. Academic Press.

- McElreath, R. (2018). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.

  - [Richard's YouTube channel](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA)

---








background-color: var(--myred)
class: inverse

.pull-left[
<img style="border-radius: 50%;" src="https://mattansb.github.io/CV/headshots/BrainOrange.jpg" width="150px"/><img src="img/BGU-logo-round-clear.png" width="20%" /><img src="img/lab_logo.png" width="20%" />

# Thank you!

### Follow me!

`r fa("twitter",,"white")` [.white[@mattansb]](https://twitter.com/mattansb) | `r fa("github",,"white")` [.white[@mattansb]](https://github.com/mattansb) | `r fa("rss",,"white")` [.white[Blog]](https://shouldbewriting.netlify.com/)

]


.pull-right[
<br><br><br><br>
.center[
<img src="img/easystats.png" width="30%" /><img src="img/bayestestR.png" width="30%" />
]

.small[
The [**`bayestestR`**](https://easystats.github.io/bayestestR) package is part of the `easystats` project. Core team members:

- Me `r emo::ji("wave")`

- Dominique Makowski ([@Dom_Makowski](https://twitter.com/Dom_Makowski))

- Daniel Lüdecke ([@strengejacke](https://twitter.com/strengejacke))

- Indrajeet Patil ([@patilindrajeets](https://twitter.com/patilindrajeets))
]
]

.footnote[
*Slides created with the R package [**`xaringan`**](https://github.com/yihui/xaringan).*
]

---









background-image: url(img/boyfriend2.jpg)
class: right, bottom

[.black[@kareem_carr]](https://twitter.com/kareem_carr/status/1356986263975395333)
